{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARJUN KUMAR\n",
    "#### karnarjun2000@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the document into DataFrame which will contain the frequency of feature words and corresponding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # importing os for extracting data\n",
    "from sklearn import model_selection # train_test_split\n",
    "# -*- coding: latin-1 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]# x is the list of text in documents\n",
    "y=[] # y is the category\n",
    "# below code for accessing the documents in which x is appended data, y is appended it's category\n",
    "for category in os.listdir(\"Downloads/20_newsgroups\"):\n",
    "    for document in os.listdir(\"Downloads/20_newsgroups/\"+category):\n",
    "        with open(\"Downloads/20_newsgroups/\"+category+\"/\"+document,\"r\") as f:\n",
    "            \n",
    "            x.append(f.read())\n",
    "            y.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating x_train,x_test, y_train and y_test randomly\n",
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(x,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing word_tokenize which will help in generating all word in document\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "#importing stopwords so that from documents, stopwords could be deleted\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# stop containing english stop words\n",
    "stop=stopwords.words('english')\n",
    "#importing string so that we can generate list of punctuation, and can remove words in documents which only contain punctuation type\n",
    "import string\n",
    "punctuations=string.punctuation\n",
    "punctuations=list(punctuations)\n",
    "# finally stop contain all symbol which should be removed if document contain it\n",
    "stop=stop+punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function produces words which will be best suited as features. Its parameter is x_train ie all documents\n",
    "def generating_feature(x_train):\n",
    "    # word_count is dictionary in which key will be word and values will be number of time this word appeared\n",
    "    word_count={}\n",
    "    # iterating over each documents\n",
    "    for i in x_train:\n",
    "        # spliting the word with the help of tokenize\n",
    "        word_split=word_tokenize(i.lower())\n",
    "        # iterating over each word, if word is appreciable then increasing it's frequecy.\n",
    "        for k in word_split:\n",
    "            # below is condtion for best type of word which can be used as feature\n",
    "            if(k not in word_count and k not in stop and len(k)!=1 and not k.isdigit() and '-' not in k and '*' not in k and '.' not in k and '+' not in k and '/' not in k and '\"\"' not in k and \"''\" not in k and \"'\" not in k ):\n",
    "                word_count[k]=1\n",
    "            elif(k not in stop and len(k)!=1 and not k.isdigit()  and '-' not in k and '*' not in k and '.' not in k and '+' not in k and '/' not in k and \"''\" not in k and '\"\"' not in k and \"'\" not in k):\n",
    "                word_count[k]+=1;\n",
    "    # converting  word_count dictionary into series, in which index is word and series it it's frequency\n",
    "    word=pd.Series(word_count)\n",
    "    freq_more_than=200 # \n",
    "    required_word=word[word>freq_more_than] # selecting top words whose frequency is greater than freq_more_than, but\n",
    "    # here we can use more lower frequency for increasing the accuracy, but running the code takes more time\n",
    "    column=list(required_word.index)\n",
    "    return column\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using generating _features for generating features\n",
    "features=generating_feature(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xref',\n",
       " 'newsgroups',\n",
       " 'path',\n",
       " 'sgiblab',\n",
       " 'psinntp',\n",
       " 'bill',\n",
       " 'subject',\n",
       " 'next',\n",
       " 'mormons',\n",
       " 'jews',\n",
       " 'organization',\n",
       " 'la',\n",
       " 'references',\n",
       " 'date',\n",
       " 'tue',\n",
       " 'apr',\n",
       " 'gmt',\n",
       " 'lines',\n",
       " 'article',\n",
       " 'jack',\n",
       " 'love',\n",
       " 'writes',\n",
       " 'branch',\n",
       " 'davidians',\n",
       " 'reality',\n",
       " 'would',\n",
       " 'likely',\n",
       " 'weapons',\n",
       " 'gotten',\n",
       " 'entirely',\n",
       " 'highly',\n",
       " 'anyone',\n",
       " 'long',\n",
       " 'see',\n",
       " 'happen',\n",
       " '``',\n",
       " 'survivors',\n",
       " 'forget',\n",
       " 'however',\n",
       " 'members',\n",
       " 'cult',\n",
       " 'responsible',\n",
       " 'atf',\n",
       " 'agents',\n",
       " 'really',\n",
       " 'expect',\n",
       " 'koresh',\n",
       " 'dead',\n",
       " 'alive',\n",
       " 'take',\n",
       " 'responsibility',\n",
       " 'trial',\n",
       " 'following',\n",
       " 'heard',\n",
       " 'two',\n",
       " 'make',\n",
       " 'right',\n",
       " 'three',\n",
       " 'rights',\n",
       " 'left',\n",
       " 'uunet',\n",
       " 'steve',\n",
       " 'question',\n",
       " 'design',\n",
       " 'systems',\n",
       " 'groups',\n",
       " 'continue',\n",
       " 'believe',\n",
       " 'christians',\n",
       " 'jehovah',\n",
       " 'argue',\n",
       " 'act',\n",
       " 'regular',\n",
       " 'service',\n",
       " 'special',\n",
       " 'paul',\n",
       " 'say',\n",
       " 'required',\n",
       " 'whether',\n",
       " 'sunday',\n",
       " 'part',\n",
       " 'god',\n",
       " 'rochester',\n",
       " 'udel',\n",
       " 'gatech',\n",
       " 'usc',\n",
       " 'power',\n",
       " 'signal',\n",
       " 'home',\n",
       " 'company',\n",
       " 'usa',\n",
       " 'mon',\n",
       " 'dave',\n",
       " 'kind',\n",
       " 'must',\n",
       " 'putting',\n",
       " 'cause',\n",
       " 'effects',\n",
       " 'equipment',\n",
       " 'feet',\n",
       " 'road',\n",
       " 'might',\n",
       " 'couple',\n",
       " 'sound',\n",
       " 'tv',\n",
       " 'direct',\n",
       " 'requires',\n",
       " 'strong',\n",
       " 'car',\n",
       " 'engine',\n",
       " 'battery',\n",
       " 'lot',\n",
       " 'sounds',\n",
       " 'like',\n",
       " 'radio',\n",
       " 'actually',\n",
       " 'completely',\n",
       " 'goes',\n",
       " 'situation',\n",
       " 'suspect',\n",
       " 'far',\n",
       " 'try',\n",
       " 'vehicle',\n",
       " 'well',\n",
       " 'low',\n",
       " 'start',\n",
       " 'extra',\n",
       " 'current',\n",
       " 'speed',\n",
       " 'drop',\n",
       " 'resolution',\n",
       " 'problem',\n",
       " 'matter',\n",
       " 'afraid',\n",
       " 'pay',\n",
       " 'much',\n",
       " 'attention',\n",
       " 'send',\n",
       " 'apply',\n",
       " 'talking',\n",
       " 'help',\n",
       " 'able',\n",
       " 'install',\n",
       " 'devices',\n",
       " 'type',\n",
       " 'could',\n",
       " 'installed',\n",
       " 'going',\n",
       " 'house',\n",
       " 'due',\n",
       " 'another',\n",
       " 'unfortunately',\n",
       " 'fair',\n",
       " 'chance',\n",
       " 'enough',\n",
       " 'simple',\n",
       " 'may',\n",
       " 'need',\n",
       " 'probably',\n",
       " 'often',\n",
       " 'internal',\n",
       " 'street',\n",
       " 'watch',\n",
       " 'cars',\n",
       " 'big',\n",
       " 'driving',\n",
       " 'time',\n",
       " 'contact',\n",
       " 'owner',\n",
       " 'person',\n",
       " 'willing',\n",
       " 'explain',\n",
       " 'cd',\n",
       " 'games',\n",
       " 'books',\n",
       " 'sale',\n",
       " 'institute',\n",
       " 'technology',\n",
       " 'plus',\n",
       " 'shipping',\n",
       " 'body',\n",
       " 'count',\n",
       " 'without',\n",
       " 'volume',\n",
       " '1st',\n",
       " 'adams',\n",
       " 'dog',\n",
       " 'music',\n",
       " 'na',\n",
       " 'ibm',\n",
       " 'pc',\n",
       " 'eye',\n",
       " 'social',\n",
       " 'history',\n",
       " 'sell',\n",
       " 'pages',\n",
       " 'physics',\n",
       " 'also',\n",
       " 'yes',\n",
       " 'offers',\n",
       " 'info',\n",
       " 'project',\n",
       " 'university',\n",
       " 'thu',\n",
       " 'file',\n",
       " 'called',\n",
       " 'think',\n",
       " 'chris',\n",
       " 'sender',\n",
       " 'news',\n",
       " 'nec',\n",
       " 'laboratory',\n",
       " 'sun',\n",
       " 'michael',\n",
       " 'onto',\n",
       " 'interesting',\n",
       " 'seen',\n",
       " 'miles',\n",
       " 'used',\n",
       " 'stuff',\n",
       " 'bike',\n",
       " 'clean',\n",
       " 'check',\n",
       " 'back',\n",
       " 'find',\n",
       " 'dealer',\n",
       " 'master',\n",
       " 'later',\n",
       " 'dod',\n",
       " 'wide',\n",
       " 'red',\n",
       " 'disclaimer',\n",
       " 'needs',\n",
       " 'name',\n",
       " 'agree',\n",
       " 'anyway',\n",
       " 'raised',\n",
       " 'user',\n",
       " 'trouble',\n",
       " 'vga',\n",
       " 'usenet',\n",
       " 'currently',\n",
       " 'true',\n",
       " 'blue',\n",
       " 'model',\n",
       " 'monitor',\n",
       " 'display',\n",
       " 'details',\n",
       " 'colors',\n",
       " 'ie',\n",
       " 'window',\n",
       " 'images',\n",
       " 'one',\n",
       " 'image',\n",
       " 'background',\n",
       " 'little',\n",
       " 'read',\n",
       " 'ago',\n",
       " 'ever',\n",
       " 'thought',\n",
       " 'mac',\n",
       " 'screen',\n",
       " 'monitors',\n",
       " 'green',\n",
       " 'hardware',\n",
       " 'solution',\n",
       " 'software',\n",
       " 'somebody',\n",
       " 'post',\n",
       " 'faq',\n",
       " 'anything',\n",
       " 'somewhere',\n",
       " 'sure',\n",
       " 'seems',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'thanks',\n",
       " 'charles',\n",
       " 'research',\n",
       " 'group',\n",
       " 'agate',\n",
       " 'uknet',\n",
       " 'pipex',\n",
       " 'peter',\n",
       " 'computer',\n",
       " 'society',\n",
       " 'david',\n",
       " 'certainly',\n",
       " 'meet',\n",
       " 'canada',\n",
       " 'final',\n",
       " 'look',\n",
       " 'last',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'eventually',\n",
       " 'lost',\n",
       " 'sera',\n",
       " 'serdar',\n",
       " 'argic',\n",
       " 'today',\n",
       " 'turkish',\n",
       " 'genocide',\n",
       " 'edt',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'month',\n",
       " 'million',\n",
       " 'muslim',\n",
       " 'people',\n",
       " 'nazi',\n",
       " 'parents',\n",
       " 'fully',\n",
       " 'european',\n",
       " 'criminal',\n",
       " 'women',\n",
       " 'children',\n",
       " 'armenia',\n",
       " 'four',\n",
       " 'years',\n",
       " 'entire',\n",
       " 'population',\n",
       " 'result',\n",
       " 'armenians',\n",
       " 'nearly',\n",
       " 'lived',\n",
       " 'soviet',\n",
       " 'armenian',\n",
       " 'government',\n",
       " 'carried',\n",
       " 'turks',\n",
       " 'found',\n",
       " 'safe',\n",
       " 'heaven',\n",
       " 'return',\n",
       " 'therefore',\n",
       " 'crime',\n",
       " 'muslims',\n",
       " 'making',\n",
       " 'determine',\n",
       " 'future',\n",
       " 'nation',\n",
       " 'come',\n",
       " 'united',\n",
       " 'states',\n",
       " 'events',\n",
       " 'political',\n",
       " 'religious',\n",
       " 'closed',\n",
       " 'roads',\n",
       " 'mountain',\n",
       " 'serve',\n",
       " 'ways',\n",
       " 'escape',\n",
       " 'work',\n",
       " 'longer',\n",
       " 'exists',\n",
       " 'single',\n",
       " 'soul',\n",
       " 'christian',\n",
       " 'heart',\n",
       " 'hedrick',\n",
       " 'state',\n",
       " 'approved',\n",
       " 'statements',\n",
       " 'questions',\n",
       " 'easily',\n",
       " 'written',\n",
       " 'reply',\n",
       " 'surface',\n",
       " 'please',\n",
       " 'accept',\n",
       " 'serious',\n",
       " 'attempt',\n",
       " 'rest',\n",
       " 'every',\n",
       " 'fun',\n",
       " 'except',\n",
       " 'eric',\n",
       " 'atheist',\n",
       " 'posting',\n",
       " 'flame',\n",
       " 'rather',\n",
       " 'express',\n",
       " 'opinion',\n",
       " 'intended',\n",
       " 'human',\n",
       " 'advanced',\n",
       " 'developed',\n",
       " 'planet',\n",
       " 'religion',\n",
       " 'appears',\n",
       " 'humans',\n",
       " 'including',\n",
       " 'seem',\n",
       " 'point',\n",
       " 'statement',\n",
       " 'made',\n",
       " 'address',\n",
       " 'life',\n",
       " 'disagree',\n",
       " 'christianity',\n",
       " 'reasons',\n",
       " 'gives',\n",
       " 'hope',\n",
       " 'purpose',\n",
       " 'safety',\n",
       " 'behind',\n",
       " 'oh',\n",
       " 'follow',\n",
       " 'moral',\n",
       " 'standard',\n",
       " 'get',\n",
       " 'eternal',\n",
       " 'many',\n",
       " 'shown',\n",
       " 'belief',\n",
       " 'various',\n",
       " 'past',\n",
       " 'sense',\n",
       " 'provides',\n",
       " 'case',\n",
       " 'points',\n",
       " 'individual',\n",
       " 'source',\n",
       " 'nothing',\n",
       " 'given',\n",
       " 'loss',\n",
       " 'control',\n",
       " 'something',\n",
       " 'else',\n",
       " 'finally',\n",
       " 'appropriate',\n",
       " 'standards',\n",
       " 'indeed',\n",
       " 'reason',\n",
       " 'save',\n",
       " 'merely',\n",
       " 'real',\n",
       " 'living',\n",
       " 'spirit',\n",
       " 'doctrine',\n",
       " 'generally',\n",
       " 'live',\n",
       " 'according',\n",
       " 'nature',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'mine',\n",
       " 'within',\n",
       " 'months',\n",
       " 'set',\n",
       " 'said',\n",
       " 'word',\n",
       " 'organizations',\n",
       " 'concerned',\n",
       " 'jesus',\n",
       " 'gave',\n",
       " 'normally',\n",
       " 'convert',\n",
       " 'cost',\n",
       " 'new',\n",
       " 'ad',\n",
       " 'done',\n",
       " 'opinions',\n",
       " 'others',\n",
       " 'ask',\n",
       " 'takes',\n",
       " 'quickly',\n",
       " 'personal',\n",
       " 'exist',\n",
       " 'easy',\n",
       " 'view',\n",
       " 'programs',\n",
       " 'attitude',\n",
       " 'worse',\n",
       " 'open',\n",
       " 'taken',\n",
       " 'advantage',\n",
       " 'bible',\n",
       " 'let',\n",
       " 'food',\n",
       " 'die',\n",
       " 'eat',\n",
       " 'capable',\n",
       " 'limited',\n",
       " 'learn',\n",
       " 'language',\n",
       " 'especially',\n",
       " 'drug',\n",
       " 'relationship',\n",
       " 'use',\n",
       " 'drugs',\n",
       " 'high',\n",
       " 'hand',\n",
       " 'even',\n",
       " 'things',\n",
       " 'lives',\n",
       " 'small',\n",
       " 'association',\n",
       " 'forces',\n",
       " 'upon',\n",
       " 'truth',\n",
       " 'realize',\n",
       " 'day',\n",
       " 'happened',\n",
       " 'large',\n",
       " 'jewish',\n",
       " 'needed',\n",
       " 'sort',\n",
       " 'keep',\n",
       " 'appear',\n",
       " 'conclusion',\n",
       " 'gone',\n",
       " 'go',\n",
       " 'thoughts',\n",
       " 'christ',\n",
       " 'know',\n",
       " 'wrong',\n",
       " 'phil',\n",
       " 'us',\n",
       " 'emory',\n",
       " 'swrinde',\n",
       " 'robert',\n",
       " 'abortion',\n",
       " 'health',\n",
       " 'clinton',\n",
       " 'fri',\n",
       " 'net',\n",
       " 'noise',\n",
       " 'illinois',\n",
       " 'urbana',\n",
       " 'scott',\n",
       " 'lots',\n",
       " 'deleted',\n",
       " 'supports',\n",
       " 'objects',\n",
       " 'care',\n",
       " 'program',\n",
       " 'everything',\n",
       " 'example',\n",
       " 'illegal',\n",
       " 'immediately',\n",
       " 'everyone',\n",
       " 'support',\n",
       " 'choice',\n",
       " 'away',\n",
       " 'feel',\n",
       " 'saying',\n",
       " 'makes',\n",
       " 'issue',\n",
       " 'main',\n",
       " 'argument',\n",
       " 'respect',\n",
       " 'greatly',\n",
       " 'ok',\n",
       " 'yeah',\n",
       " 'order',\n",
       " 'tax',\n",
       " 'money',\n",
       " 'luck',\n",
       " 'first',\n",
       " 'place',\n",
       " 'want',\n",
       " 'top',\n",
       " 'costs',\n",
       " 'ca',\n",
       " 'include',\n",
       " 'suppose',\n",
       " 'instead',\n",
       " 'process',\n",
       " 'paid',\n",
       " 'remove',\n",
       " 'legal',\n",
       " 'notice',\n",
       " 'someone',\n",
       " 'larger',\n",
       " 'bunch',\n",
       " 'related',\n",
       " 'majority',\n",
       " 'win',\n",
       " 'fact',\n",
       " 'says',\n",
       " 'number',\n",
       " 'country',\n",
       " 'important',\n",
       " 'personally',\n",
       " 'stupid',\n",
       " 'vote',\n",
       " 'consider',\n",
       " 'way',\n",
       " 'issues',\n",
       " 'seriously',\n",
       " 'media',\n",
       " 'nice',\n",
       " 'middle',\n",
       " 'side',\n",
       " 'course',\n",
       " 'answer',\n",
       " 'president',\n",
       " 'policy',\n",
       " 'review',\n",
       " 'still',\n",
       " 'understanding',\n",
       " 'position',\n",
       " 'object',\n",
       " 'already',\n",
       " 'yet',\n",
       " 'none',\n",
       " 'business',\n",
       " 'summary',\n",
       " 'vnews',\n",
       " 'keywords',\n",
       " 'product',\n",
       " 'division',\n",
       " 'correct',\n",
       " 'quite',\n",
       " 'reports',\n",
       " 'via',\n",
       " 'office',\n",
       " 'call',\n",
       " 'guess',\n",
       " 'sex',\n",
       " 'national',\n",
       " 'starting',\n",
       " 'secret',\n",
       " 'kill',\n",
       " 'thousands',\n",
       " 'using',\n",
       " 'calling',\n",
       " 'obvious',\n",
       " 'lies',\n",
       " 'present',\n",
       " 'evidence',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'building',\n",
       " 'around',\n",
       " 'draw',\n",
       " 'picture',\n",
       " 'absolutely',\n",
       " 'whatever',\n",
       " 'johnson',\n",
       " 'wait',\n",
       " 'douglas',\n",
       " 'wupost',\n",
       " 'rutgers',\n",
       " 'tim',\n",
       " 'law',\n",
       " 'works',\n",
       " 'scripture',\n",
       " 'convex',\n",
       " 'george',\n",
       " 'univ',\n",
       " 'texas',\n",
       " 'wed',\n",
       " 'interested',\n",
       " 'trying',\n",
       " 'method',\n",
       " 'went',\n",
       " 'doctor',\n",
       " 'june',\n",
       " 'looks',\n",
       " 'per',\n",
       " 'week',\n",
       " 'followed',\n",
       " 'shot',\n",
       " 'somewhat',\n",
       " 'getting',\n",
       " 'information',\n",
       " 'public',\n",
       " 'library',\n",
       " 'page',\n",
       " 'reading',\n",
       " 'mike',\n",
       " 'microsoft',\n",
       " 'nntp',\n",
       " 'poster',\n",
       " 'views',\n",
       " 'necessarily',\n",
       " 'development',\n",
       " 'common',\n",
       " 'shell',\n",
       " 'previous',\n",
       " 'versions',\n",
       " 'wrote',\n",
       " 'windows',\n",
       " 'code',\n",
       " 'difference',\n",
       " 'running',\n",
       " 'possible',\n",
       " 'mode',\n",
       " 'dos',\n",
       " 'box',\n",
       " 'since',\n",
       " 'ability',\n",
       " 'run',\n",
       " 'added',\n",
       " 'everybody',\n",
       " 'internet',\n",
       " 'official',\n",
       " 'gary',\n",
       " 'kept',\n",
       " 'hours',\n",
       " 'wonder',\n",
       " 'hard',\n",
       " 'add',\n",
       " 'machines',\n",
       " 'built',\n",
       " 'machine',\n",
       " 'amount',\n",
       " 'apple',\n",
       " 'governments',\n",
       " 'countries',\n",
       " 'require',\n",
       " 'companies',\n",
       " 'goals',\n",
       " 'quote',\n",
       " 'note',\n",
       " 'load',\n",
       " 'air',\n",
       " 'study',\n",
       " 'simply',\n",
       " 'night',\n",
       " 'year',\n",
       " 'automatic',\n",
       " 'features',\n",
       " 'greater',\n",
       " 'double',\n",
       " 'value',\n",
       " 'seeing',\n",
       " 'leave',\n",
       " 'computers',\n",
       " 'email',\n",
       " 'potential',\n",
       " 'cut',\n",
       " 'based',\n",
       " 'data',\n",
       " 'btw',\n",
       " 'pointed',\n",
       " 'switch',\n",
       " 'front',\n",
       " 'designed',\n",
       " 'worth',\n",
       " 'daily',\n",
       " 'wayne',\n",
       " 'light',\n",
       " 'fire',\n",
       " 'reported',\n",
       " 'etc',\n",
       " 'several',\n",
       " 'bd',\n",
       " 'folks',\n",
       " 'places',\n",
       " 'rules',\n",
       " 'interest',\n",
       " 'blame',\n",
       " 'discussion',\n",
       " 'proper',\n",
       " 'problems',\n",
       " 'watching',\n",
       " 'fbi',\n",
       " 'actions',\n",
       " 'outside',\n",
       " 'compound',\n",
       " 'independent',\n",
       " 'sources',\n",
       " 'different',\n",
       " 'hardly',\n",
       " 'action',\n",
       " 'claim',\n",
       " 'dangerous',\n",
       " 'general',\n",
       " 'fan',\n",
       " 'ahead',\n",
       " 'full',\n",
       " 'tell',\n",
       " 'job',\n",
       " 'speak',\n",
       " 'americans',\n",
       " 'cramer',\n",
       " 'clayton',\n",
       " 'james',\n",
       " 'cheap',\n",
       " 'gun',\n",
       " 'constitution',\n",
       " 'batf',\n",
       " 'expressed',\n",
       " 'major',\n",
       " 'hold',\n",
       " 'gld',\n",
       " 'insurance',\n",
       " 'ii',\n",
       " 'network',\n",
       " 'hall',\n",
       " 'daniel',\n",
       " 'guy',\n",
       " 'came',\n",
       " 'disease',\n",
       " 'seattle',\n",
       " 'american',\n",
       " 'private',\n",
       " 'cover',\n",
       " 'whole',\n",
       " 'treatment',\n",
       " 'buy',\n",
       " 'local',\n",
       " 'rates',\n",
       " 'coming',\n",
       " 'fall',\n",
       " 'list',\n",
       " 'necessary',\n",
       " 'thinking',\n",
       " 'practice',\n",
       " 'hospital',\n",
       " 'figure',\n",
       " 'advance',\n",
       " 'operations',\n",
       " 'applied',\n",
       " 'give',\n",
       " 'brother',\n",
       " 'town',\n",
       " 'table',\n",
       " 'less',\n",
       " 'book',\n",
       " 'toronto',\n",
       " 'british',\n",
       " 'columbia',\n",
       " 'boston',\n",
       " 'head',\n",
       " 'market',\n",
       " 'size',\n",
       " 'provide',\n",
       " 'services',\n",
       " 'toward',\n",
       " 'noted',\n",
       " 'somehow',\n",
       " 'canadian',\n",
       " 'free',\n",
       " 'buying',\n",
       " 'bob',\n",
       " 'anybody',\n",
       " 'offer',\n",
       " 'basic',\n",
       " 'coverage',\n",
       " 'remember',\n",
       " 'hear',\n",
       " 'recently',\n",
       " 'proposed',\n",
       " 'put',\n",
       " 'effort',\n",
       " 'wanted',\n",
       " 'medicine',\n",
       " 'plan',\n",
       " 'particular',\n",
       " 'option',\n",
       " 'across',\n",
       " 'north',\n",
       " 'effective',\n",
       " 'turn',\n",
       " 'pat',\n",
       " 'started',\n",
       " 'became',\n",
       " 'ones',\n",
       " 'western',\n",
       " 'des',\n",
       " 'store',\n",
       " 'ontario',\n",
       " 'trade',\n",
       " 'saw',\n",
       " 'old',\n",
       " 'administration',\n",
       " 'knew',\n",
       " 'always',\n",
       " 'taxes',\n",
       " 'press',\n",
       " 'party',\n",
       " 'april',\n",
       " 'mail',\n",
       " 'letter',\n",
       " 'california',\n",
       " 'results',\n",
       " 'proof',\n",
       " 'recall',\n",
       " 'hit',\n",
       " 'function',\n",
       " 'period',\n",
       " 'former',\n",
       " 'patients',\n",
       " 'among',\n",
       " 'area',\n",
       " 'resources',\n",
       " 'either',\n",
       " 'bad',\n",
       " 'bought',\n",
       " 'deal',\n",
       " 'changes',\n",
       " 'medical',\n",
       " 'resource',\n",
       " 'rate',\n",
       " 'sick',\n",
       " 'training',\n",
       " 'tend',\n",
       " 'cold',\n",
       " 'education',\n",
       " 'ah',\n",
       " 'spend',\n",
       " 'buffalo',\n",
       " 'game',\n",
       " 'espn',\n",
       " 'ny',\n",
       " 'story',\n",
       " 'grant',\n",
       " 'sorry',\n",
       " 'considered',\n",
       " 'employer',\n",
       " 'lower',\n",
       " 'mean',\n",
       " 'create',\n",
       " 'decide',\n",
       " 'approach',\n",
       " 'germany',\n",
       " 'areas',\n",
       " 'studies',\n",
       " 'west',\n",
       " 'great',\n",
       " 'pretty',\n",
       " 'close',\n",
       " 'better',\n",
       " 'flight',\n",
       " 'pittsburgh',\n",
       " 'increase',\n",
       " 'alternative',\n",
       " 'friends',\n",
       " 'end',\n",
       " 'choose',\n",
       " 'america',\n",
       " 'directly',\n",
       " 'involved',\n",
       " 'military',\n",
       " 'term',\n",
       " 'means',\n",
       " 'gets',\n",
       " 'tech',\n",
       " 'comes',\n",
       " 'become',\n",
       " 'values',\n",
       " 'nobody',\n",
       " 'thank',\n",
       " 'hockey',\n",
       " 'seemed',\n",
       " 'told',\n",
       " 'showed',\n",
       " 'manager',\n",
       " 'bank',\n",
       " 'length',\n",
       " 'certain',\n",
       " 'access',\n",
       " 'bet',\n",
       " 'talk',\n",
       " 'cable',\n",
       " 'bit',\n",
       " '==',\n",
       " 'jim',\n",
       " 'mit',\n",
       " 'lab',\n",
       " 'lee',\n",
       " 'passed',\n",
       " 'along',\n",
       " 'regarding',\n",
       " 'version',\n",
       " 'parts',\n",
       " 'typical',\n",
       " 'historical',\n",
       " 'document',\n",
       " 'appreciate',\n",
       " 'arguments',\n",
       " 'event',\n",
       " 'perhaps',\n",
       " 'recent',\n",
       " 'search',\n",
       " 'decwrl',\n",
       " 'van',\n",
       " 'drive',\n",
       " 'second',\n",
       " 'session',\n",
       " 'write',\n",
       " 'looking',\n",
       " 'directory',\n",
       " 'change',\n",
       " 'older',\n",
       " 'taking',\n",
       " 'multiple',\n",
       " 'account',\n",
       " 'john',\n",
       " 'strange',\n",
       " 'los',\n",
       " 'totally',\n",
       " 'graphics',\n",
       " 'although',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features # printing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1926"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)# printing the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create the DataFrame names in which each row has the frequecy of feature word for each document.\n",
    "# This function has features and document as parameter\n",
    "def prefitting(features,x):\n",
    "    #creating dictionary which will contain each feature word frequency in each document.Each feature is key \n",
    "    dictionary={}\n",
    "    for i in features:\n",
    "        dictionary[i]=[0]*len(x)\n",
    "    for i in range(len(x)):\n",
    "        text=x[i]\n",
    "        for j in word_tokenize(text.lower()):\n",
    "            if(j in features):\n",
    "                dictionary[j][i]+=1\n",
    "    dataframe=pd.DataFrame(dictionary)\n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe with functin prefitting for documents. This dataFrame conatins word as feature and rows as it's frequency\n",
    "x_train_final=prefitting(features,x_train)\n",
    "x_test_final=prefitting(features,x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xref</th>\n",
       "      <th>newsgroups</th>\n",
       "      <th>path</th>\n",
       "      <th>sgiblab</th>\n",
       "      <th>psinntp</th>\n",
       "      <th>bill</th>\n",
       "      <th>subject</th>\n",
       "      <th>next</th>\n",
       "      <th>mormons</th>\n",
       "      <th>jews</th>\n",
       "      <th>...</th>\n",
       "      <th>wm</th>\n",
       "      <th>giz</th>\n",
       "      <th>6um</th>\n",
       "      <th>1d9</th>\n",
       "      <th>q,3</th>\n",
       "      <th>7ey</th>\n",
       "      <th>0d</th>\n",
       "      <th>a86</th>\n",
       "      <th>b8f</th>\n",
       "      <th>buf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14997 rows Ã— 1926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       xref  newsgroups  path  sgiblab  psinntp  bill  subject  next  mormons  \\\n",
       "0         1           1     1        1        2     1        1     1        1   \n",
       "1         0           1     1        0        0     0        1     0        0   \n",
       "2         1           1     1        0        0     2        1     0        0   \n",
       "3         0           1     1        0        0     0        1     0        0   \n",
       "4         0           1     1        0        0     0        1     0        0   \n",
       "...     ...         ...   ...      ...      ...   ...      ...   ...      ...   \n",
       "14992     0           1     1        0        0     0        1     1        0   \n",
       "14993     1           1     1        0        0     0        1     0        2   \n",
       "14994     0           1     1        0        0     0        1     0        0   \n",
       "14995     0           1     1        0        0     0        1     0        0   \n",
       "14996     0           1     1        0        0     0        1     0        0   \n",
       "\n",
       "       jews  ...  wm  giz  6um  1d9  q,3  7ey  0d  a86  b8f  buf  \n",
       "0         1  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "1         0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "2         0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "3         0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4         0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "...     ...  ...  ..  ...  ...  ...  ...  ...  ..  ...  ...  ...  \n",
       "14992     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "14993     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "14994     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "14995     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "14996     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "\n",
       "[14997 rows x 1926 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_final # shoing the DataFrame for word and it's frequecy in each documents. This is for training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xref</th>\n",
       "      <th>newsgroups</th>\n",
       "      <th>path</th>\n",
       "      <th>sgiblab</th>\n",
       "      <th>psinntp</th>\n",
       "      <th>bill</th>\n",
       "      <th>subject</th>\n",
       "      <th>next</th>\n",
       "      <th>mormons</th>\n",
       "      <th>jews</th>\n",
       "      <th>...</th>\n",
       "      <th>wm</th>\n",
       "      <th>giz</th>\n",
       "      <th>6um</th>\n",
       "      <th>1d9</th>\n",
       "      <th>q,3</th>\n",
       "      <th>7ey</th>\n",
       "      <th>0d</th>\n",
       "      <th>a86</th>\n",
       "      <th>b8f</th>\n",
       "      <th>buf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 1926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      xref  newsgroups  path  sgiblab  psinntp  bill  subject  next  mormons  \\\n",
       "0        0           1     1        0        0     0        1     0        0   \n",
       "1        1           1     1        0        0     0        1     0        0   \n",
       "2        0           1     1        0        0     0        1     1        0   \n",
       "3        0           1     1        0        0     0        1     0        0   \n",
       "4        0           1     1        0        0     1        1     1        0   \n",
       "...    ...         ...   ...      ...      ...   ...      ...   ...      ...   \n",
       "4995     0           1     1        0        0     0        1     0        0   \n",
       "4996     0           1     1        0        0     0        1     0        0   \n",
       "4997     0           1     1        0        0     1        1     0        0   \n",
       "4998     1           1     1        0        0     0        1     0        0   \n",
       "4999     0           1     1        0        0     0        1     0        0   \n",
       "\n",
       "      jews  ...  wm  giz  6um  1d9  q,3  7ey  0d  a86  b8f  buf  \n",
       "0        0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "1        0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "2        0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "3        0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4        0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "...    ...  ...  ..  ...  ...  ...  ...  ...  ..  ...  ...  ...  \n",
       "4995     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4996     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4997     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4998     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "4999     0  ...   0    0    0    0    0    0   0    0    0    0  \n",
       "\n",
       "[5000 rows x 1926 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_final  # shoing the DataFrame for word and it's frequecy in each documents. This is for training documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have DataFrame Which contains the frequency of certain word(features) names as x_train_final and x_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now implement pre- made MultinomialNB in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # importing Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB() # creating class as multinomial\n",
    "#importing classification_report and confusion_matrix to show my accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.69      0.77      0.73       233\n",
      "           comp.graphics       0.61      0.65      0.63       253\n",
      " comp.os.ms-windows.misc       0.88      0.31      0.45       249\n",
      "comp.sys.ibm.pc.hardware       0.52      0.69      0.59       240\n",
      "   comp.sys.mac.hardware       0.57      0.79      0.66       236\n",
      "          comp.windows.x       0.73      0.64      0.68       240\n",
      "            misc.forsale       0.67      0.85      0.75       261\n",
      "               rec.autos       0.76      0.77      0.77       269\n",
      "         rec.motorcycles       0.68      0.88      0.77       284\n",
      "      rec.sport.baseball       0.84      0.83      0.84       248\n",
      "        rec.sport.hockey       0.89      0.90      0.89       231\n",
      "               sci.crypt       0.93      0.86      0.89       233\n",
      "         sci.electronics       0.67      0.68      0.67       244\n",
      "                 sci.med       0.87      0.75      0.81       256\n",
      "               sci.space       0.87      0.83      0.85       246\n",
      "  soc.religion.christian       0.92      0.98      0.95       252\n",
      "      talk.politics.guns       0.69      0.81      0.74       249\n",
      "   talk.politics.mideast       0.92      0.77      0.84       281\n",
      "      talk.politics.misc       0.68      0.58      0.63       259\n",
      "      talk.religion.misc       0.63      0.37      0.47       236\n",
      "\n",
      "                accuracy                           0.74      5000\n",
      "               macro avg       0.75      0.74      0.73      5000\n",
      "            weighted avg       0.75      0.74      0.73      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train_final,y_train) # training clf function  from training data\n",
    "y_pred=clf.predict(x_test_final)# predicting for testing data\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179   1   0   0   0   0   0   2   7   2   1   0   0   3   0   4   1   1\n",
      "    4  28]\n",
      " [  1 165   5  25  17  13   6   3   6   0   1   0   4   3   3   0   0   0\n",
      "    0   1]\n",
      " [  0  29  76  70  17  34   9   1   2   0   0   3   2   1   4   0   1   0\n",
      "    0   0]\n",
      " [  0   4   1 166  38   2  12   5   2   0   0   1   9   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0  23 186   0  12   2   3   0   0   0   8   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0  37   3  12   9 154   7   1   2   3   0   2   9   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0  10   2   2 222   7   3   0   2   0   9   0   3   0   0   0\n",
      "    1   0]\n",
      " [  0   3   0   2   7   1  14 208  19   2   0   0  10   0   1   0   1   0\n",
      "    0   1]\n",
      " [  1   0   0   0   1   0   9  12 250   2   1   0   3   0   0   0   2   0\n",
      "    3   0]\n",
      " [  0   3   0   0   3   0   7   4   8 206  14   0   1   0   2   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   6  15 207   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   1   0   2   3   2   1   2   0   0 200   6   1   1   0   4   0\n",
      "    6   0]\n",
      " [  0  12   0  13  25   1   9   8   3   1   0   2 166   2   2   0   0   0\n",
      "    0   0]\n",
      " [  3   5   0   0   7   0   4   5  15   3   3   1   9 193   5   0   1   1\n",
      "    0   1]\n",
      " [  0   5   0   0   2   1   2   3  10   2   1   0   4   4 203   0   2   0\n",
      "    6   1]\n",
      " [  1   0   0   0   0   1   0   0   0   0   0   0   0   0   0 248   0   1\n",
      "    1   0]\n",
      " [  2   0   0   0   1   0   5   3   5   3   0   3   3   0   1   0 201   2\n",
      "   14   6]\n",
      " [ 10   0   0   0   5   0   6   3  11   2   0   0   3   2   0   0   4 217\n",
      "   14   4]\n",
      " [  5   1   0   1   2   0   3   4  10   2   0   3   0   7   5   0  45  11\n",
      "  150  10]\n",
      " [ 57   1   0   0   0   0   1   0   4   1   3   1   1   5   3  19  28   4\n",
      "   20  88]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))# printing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844235513769421"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train_final,y_train) # finding the training score which got increased on increasing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.737"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test_final,y_test)# finding the testing score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now applying naive bayes to finding the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function create a  dictionary in which each category is the key, and it's values are to number of time word appeared for this category,and number of time feature word appeared\n",
    "def fit(X_train, Y_train):\n",
    "    # result is mentioned above dictionary\n",
    "    result = {}\n",
    "    # class_values is different type of category\n",
    "    class_values = set(Y_train)\n",
    "    # result[\"total_data\"] is the length of y_train\n",
    "    result[\"total_data\"] = len(Y_train)\n",
    "    # result[\"total_number_of_word\"] is the total number of times each featured word appeared\n",
    "    result[\"total_number_of_word\"]=(X_train.sum()).sum()\n",
    "    # iterating over each classes and each class as key contains its the number of word appeared for each features, and it's total sum.\n",
    "    for current_class in class_values:\n",
    "        # result[current_class] is a dictionary\n",
    "        result[current_class] = {}\n",
    "        # current_class_rows is the serees of true or false for Y_train=current_class\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        # X_train_current is the dataframe where current_class_rows is true\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        # Y_train_current is the dataframe where current_class_rows is true\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        # result[current_class][\"total_count\"] is the total count of word where current_class is present\n",
    "        result[current_class][\"total_count\"] =(X_train_current.sum()).sum()\n",
    "        #It iterate overr range of num feature for each features word\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = X_train_current.iloc[:,j-1].sum()\n",
    "    # returning result which is dictionary\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function return the probability to be current  for row x \n",
    "def probability(dictionary, x, current_class):\n",
    "    # here ouput is the probability to current class out of total length of dataframe\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    #here num_features is the length of x ie number of features\n",
    "    num_features = len(x)\n",
    "    # here iterating from 1 to num_features, and applying formula to find probability\n",
    "    for j in range(1, num_features + 1):\n",
    "        xj = x[j - 1]# jth term of x\n",
    "        \n",
    "        if(xj!=0):# xj==0 then igoner it, if not then \n",
    "            # here below variable is the number of total number of word in jth feature which is related to current_class\n",
    "            # here also using laplace correction\n",
    "            count_current_class_with_value_xj = dictionary[current_class][j] + 1\n",
    "            # here this variable is giveing total number of words which is related to current_class added with total number of words which is used for features\n",
    "            count_current_class = dictionary[current_class][\"total_count\"] + dictionary[\"total_data\"]\n",
    "            # here we using log of probability\n",
    "            current_xj_probablity = (np.log(count_current_class_with_value_xj) - np.log(count_current_class))\n",
    "            output = output + current_xj_probablity\n",
    "    # returning output  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function return the best category related to single point \n",
    "def predictSinglePoint(dictionary, x):\n",
    "    # here classes is the all category that was put in as dictionary key\n",
    "    classes = dictionary.keys()\n",
    "    # best_p will contains the best probability for the best_class, here assuming to be -1000\n",
    "    best_p = -1000\n",
    "    # best_class will contain the name of best class out of total class, here best_class assuming to be -1\n",
    "    best_class = -1\n",
    "    first_run = True  #his variable taking care of first run\n",
    "    # iterating over each class and finding the probabilty according to the class \n",
    "    #nd if probability coming better then assigning it to best_p, and the class is assigned to best_class\n",
    "    for current_class in classes:\n",
    "        # as iterating over current_class which come from dictionary contains total_data and total_number of_word which is no any category,\n",
    "        # so ignoring\n",
    "        if (current_class == \"total_data\" or current_class==\"total_number_of_word\"):\n",
    "            continue\n",
    "        # p_current_class contains the probability to be current_class for x datapoint\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        # assigning better probability to best_p and better class to best_class\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    #returning best_class\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now finally predicting the category for each data point for test dataFrame\n",
    "def predict(dictionary, X_test):\n",
    "    # y_pred contains the categroy related to each datapoint in X_test\n",
    "    y_pred = []\n",
    "    X_test=X_test.values\n",
    "    #iterating over each data point in X_test\n",
    "    for x in X_test:\n",
    "        # for each data point finding the best class\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        # appending the best class to y_pred\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the fit function to create dictionary as mentioned above\n",
    "dictionary = fit(x_train_final,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using predict method to predict the category of each document \n",
    "y_pred = predict(dictionary,x_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.69      0.79      0.74       233\n",
      "           comp.graphics       0.54      0.66      0.59       253\n",
      " comp.os.ms-windows.misc       0.88      0.18      0.29       249\n",
      "comp.sys.ibm.pc.hardware       0.51      0.73      0.60       240\n",
      "   comp.sys.mac.hardware       0.66      0.72      0.69       236\n",
      "          comp.windows.x       0.69      0.68      0.69       240\n",
      "            misc.forsale       0.74      0.80      0.77       261\n",
      "               rec.autos       0.74      0.80      0.77       269\n",
      "         rec.motorcycles       0.82      0.82      0.82       284\n",
      "      rec.sport.baseball       0.82      0.84      0.83       248\n",
      "        rec.sport.hockey       0.90      0.87      0.88       231\n",
      "               sci.crypt       0.86      0.88      0.87       233\n",
      "         sci.electronics       0.64      0.69      0.66       244\n",
      "                 sci.med       0.87      0.76      0.81       256\n",
      "               sci.space       0.81      0.86      0.83       246\n",
      "  soc.religion.christian       0.95      0.99      0.97       252\n",
      "      talk.politics.guns       0.70      0.78      0.74       249\n",
      "   talk.politics.mideast       0.91      0.77      0.84       281\n",
      "      talk.politics.misc       0.58      0.65      0.61       259\n",
      "      talk.religion.misc       0.64      0.38      0.47       236\n",
      "\n",
      "                accuracy                           0.73      5000\n",
      "               macro avg       0.75      0.73      0.72      5000\n",
      "            weighted avg       0.75      0.73      0.73      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# give the report of precision etc.\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185   1   0   0   0   0   0   1   4   3   1   0   1   3   0   0   2   1\n",
      "    5  26]\n",
      " [  2 168   4  23   8  13   6   3   3   1   0   0   7   4   9   0   0   0\n",
      "    0   2]\n",
      " [  0  50  44  69  10  46   7   1   0   1   0   5   8   1   5   0   1   0\n",
      "    1   0]\n",
      " [  0   7   0 175  22   3  11   5   1   0   1   1  14   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0  31 170   0  10   2   2   0   0   1  13   0   1   0   2   0\n",
      "    1   0]\n",
      " [  0  40   2   6   5 164   6   2   1   2   0   3   5   2   1   0   0   0\n",
      "    1   0]\n",
      " [  0   2   0  14   3   0 209   9   1   1   0   1  12   0   5   0   0   0\n",
      "    4   0]\n",
      " [  0   3   0   3   7   1   9 215   8   0   0   0  10   1   4   0   3   0\n",
      "    5   0]\n",
      " [  2   1   0   0   1   1   6  24 234   2   0   1   3   1   1   0   2   0\n",
      "    5   0]\n",
      " [  1   3   0   1   5   0   3   3   3 209  12   0   3   1   3   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   1   0   4  22 200   0   1   0   1   0   0   0\n",
      "    2   0]\n",
      " [  0   6   0   1   0   5   0   1   1   0   0 204   1   2   3   0   4   0\n",
      "    5   0]\n",
      " [  0  15   0  16  13   1   4   9   3   1   1   5 168   1   2   0   4   1\n",
      "    0   0]\n",
      " [  5   8   0   0   5   1   3   3   8   4   2   1   7 195   6   0   3   1\n",
      "    1   3]\n",
      " [  0   4   0   0   2   1   3   3   2   2   0   0   4   2 212   0   1   0\n",
      "    9   1]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0   0   1   0 249   0   0\n",
      "    1   0]\n",
      " [  2   0   0   0   1   0   2   2   3   1   1   7   0   0   0   0 195   2\n",
      "   27   6]\n",
      " [  7   0   0   0   4   0   3   4   4   2   0   0   4   2   2   0   4 217\n",
      "   24   4]\n",
      " [  5   1   0   2   0   0   0   3   3   2   0   5   1   5   6   0  35  14\n",
      "  168   9]\n",
      " [ 61   1   0   0   0   0   1   2   2   1   3   3   1   3   2  12  22   2\n",
      "   31  89]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "   We can see that Multinomial and own made naive bayes are giving the approximately same result, ie 75% accuracy, which is better.\n",
    "   Care can be taken that accuracy can be increased by increaseing the features of DataFrame, but will take time according for \n",
    "   running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
